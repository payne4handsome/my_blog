---
title: "MMICL"
date: 2023-10-15T17:58:18+08:00
draft: false
categories: [paper]
tags: []
author: pan
---
1. Title:
2. 作者:
3. 发表日期:

## 一、Introduction

### 1.1 该论文试图解决什么问题？

LLM可以通过in-context learning利用背景信息和任务信息，然而，VLM还很难理解多张图片的多模prompt。之前的很多工作只能处理单张图片，尽管已经存在可以处理多张图片的多模模型，但是其预训练数据的prompt不够老练（sophisticated）。本文提出MMICL， 从模型设计和数据两个方面去解决这个问题（训练的数据和真实应用场景的数据存在gap）。
这个gap表现为：

1. 图片和文本交错的多模上下文
2. 图片的文本指代
3. 多模数据存在空间、逻辑、时间关系

当前VLM存在的现状

1. Hard to Understand Complex Prompt With Multiple Images and Text
   难以理解包含多张图片且图片与文本相互交错的复杂问题。虽然Flamingo可以处理多张图片，但是其预训练数据的prompt不过老练（sophisticated）
2. Hard to Understand Text-to-Image Reference
   很难理解问题问的哪张图片
3. Hard to Understand the Relationships between Multiple Images
   之前用的训练数据是从网上爬取的，虽然来自同一个页面，但是图片间的联系可能是比较弱的。图片之间缺乏联系（interconnected）阻碍VLM理解多张图片之间的复杂关系（空间、时间、逻辑关系），其进一步限制了模型的推理能力和few-shot能力

### 1.2 Key Contributions

1. 提出方法MMICL， 可以有效的处理多模输入（包括多张图片的关系和文本到图片的指代）
2. 提出新的上下文方案（an extra image declaration section and image proxy tokens）增强VLM的上写文学习能力
3. 构建MIC（Multi-modal In-Context）数据集
此外，MMICL可以缓解语言的偏见（language bias），广泛语境下language bias会导致幻觉问题

## Method

### Experiments

