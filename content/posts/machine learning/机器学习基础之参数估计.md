---
layout: post
title: "机器学习基础之参数估计"
date: 2023-09-04
# menu: main
categories: [机器学习]
tags: [参数估计]
---

# 机器学习基础之参数估计

## 一、参数估计

对所要研究的随机变量$\xi$，当它的概率分布的类型已知，但是参数未知，比如$\xi$服从正太分布$N(\alpha, \sigma)$。但是$\alpha, \sigma$这两个参数未知。那么这么确定这些未知参数呢？我们可以**通过采样的方式，得到一批样本数据，用样本的统计量来估计总体的统计量。那么这种方式就是参数估计**。

我们先来看一种简单的估计。

**矩法估计**：设总体$\xi$的分布函数$F(x; \theta_1,\theta_2, ..., \theta_l)$中$l$个未知参数$\theta_1,\theta_2, ..., \theta_l$。假定总体$\xi$的$l$阶原点绝对矩有限，并记$v_k=E(\xi^k)  (k=1,2,...,l)$。现用样本的k阶原点矩来作为总体的k阶矩的估计量$\hat{v}_k$。即
$v_k=\hat{v}_k=\frac{1}{n}\sum_{i=1}^n\xi_i^k$

那么通过样本的估计量，我们就可以估计出总体的一些参数。

比如假设$\xi$服从一个分布（不管什么分布），$E(\xi)=\alpha, D(\xi)=\sigma^2$。但其值未知，则由样本的一阶矩、二阶矩

$\hat{v}_1=\frac{1}{n}\sum_{i=1}^n\xi_i=\overline{\xi}$

$\hat{v}_2=\frac{1}{n}\sum_{i=1}^n\xi^2_i$

总体的一阶矩、二阶矩

$v_1=E(\xi^1)=\alpha, v_2=E(\xi^2)=D(\xi)+(E(\xi))^2=\sigma^2+\alpha^2$

令$v_1=\hat{v}_1, v_2=\hat{v}_2$, 就可以解出参数$\alpha, \sigma$的值.

$\hat{\alpha}=\overline{\xi}\\
\hat{\sigma^2}=\frac{1}{n}\sum_{i=1}^n\xi^2_i-(\overline{\xi}^2)=\frac{1}{n}\sum_{i=1}^n(\xi_i-\overline{\xi})^2=S^2$

## 二、极大似然估计（Maximum Likelihood Estimate）

**矩法估计要求随机变量$\xi$的原点矩存在。再者，样本矩的表达式用总体$\xi$的分布函数表达式无关，因此矩法估计没有充分利用分布函数对参数提供的信息**。所以很多时候我们采用极大似然估计

（极大似然估计）设总体的$\xi$的密度函数为$f(x;\theta_1, \theta_2, ..., \theta_l)$，其中$\theta_1, \theta_2, ..., \theta_l$为未知参数。$\xi_1, \xi_2, ..., \xi_n$为样本，它的联合密度函数为$f(x_1, x_2, ..., x_n;\theta_1, \theta_2, ..., \theta_l)$。 
称

$L(\theta_1, \theta_2, ..., \theta_l)=\prod_{i=1}^nf(x_i; \theta_1, \theta_2, ..., \theta_l)$为$\theta_1, \theta_2, ..., \theta_l$的似然函数。若有$\hat{\theta_1}, \hat{\theta_2}, ..., \hat{\theta_l}$使得下试成立：

$L(\hat{\theta_1}, \hat{\theta_2}, ..., \hat{\theta_l})=max  {L(\theta_1, \theta_2, ..., \theta_l)}$, 则称$\hat{\theta_1}, \hat{\theta_2}, ..., \hat{\theta_l}$为为参数$\theta_1, \theta_2, ..., \theta_l$的极大似然估计量

举例：
假如有一个罐子，里面有黑白两种颜色的球。我们独立且有放回的取100次，统计得到70个白球，30个黑球。那么我们**凭感觉**可以猜测这个罐子里白球占70%，黑色占30%。假设取得一次白球的概率为p,那么这次实验的数学表达就是

$P(该次实验)=p^{70}(1-p)^{30}$

我们有理由相信我们**观察到的结果是概率最大的**。 所以对上述式子求导，可以得到当p=0.7时取得最大值。

**所以极大似然背后的直观原理就是我们观测到的实验结果是概率最大的**

## 三、再谈逻辑回归

训练数据${(x_1,y_1), (x_2, y_2), ...,(x_N, y_N)}$, 其中$x_i \in R^n, y_i \in {0,1}$。即每个样本$x_i$有n个特征，标签1表示正例、0表示负例。逻辑回归模型描述如下：

$z = wx+b\\
a=\sigma(z)\\
P(Y=1;w)=a, P(Y=0;w)=1-a$,

其中$w\in R^n$是需要学习的参数,$\sigma=\frac{1}{1+e^(-x)}$是激活函数。

数据已知，参数$w$未知，概率分布已知。那么就可以极大似然估计来估计模型参数。

$L(w)=\prod_{i=1}^N(a_w(x_i))^{y_i}(1-a_w(x_i))^{1-y_i}$, 其中$a_w(x_i))$表示在输入是$x_i$时候的模型输出。
**模型的训练目标就是找到参数w使得上述似然函数取得最大值**。那么这么找到这个w呢？
**通过反向传播算法让w沿梯度正方向更新**
去对数不改变函数取得最大值时的w，所以在实际过程中都是用的对数似然。

$ln(L(w))=\sum_{i=1}^N[y_iln(a(x_i))+(1-y_i)ln(1-a(x_i))]$

$\frac{\partial(ln(L(w)))}{\partial(w)}=\sum_{i=1}^N[y_i*\frac{1}{a(x_i)}*a(x_i)*(1-a(x_i))*x_i+(1-y_i)*\frac{1}{1-a(x_i)}*(-1)*a(x_i)*(1-a(x_i))*x_i]=\\
\sum_{i=1}^N[yi*(1-a(x_i))*x_i-(1-y_i)*a(x_i)*x_i]=\sum_{i=1}^N[x_i*y_i-x_i*a(x_i)]=\\
\sum_{i=1}^N[x_i(yi-a(x_i))]
$
其中$a(x_i)$在前向传播过程中是已知的，所以这个表达式还是很简洁的。

注：上述求导过程中用到了链式法则和$\frac{\partial \sigma}{\partial x}=\sigma(1-\sigma)$公式

参考文献：
1. 邓集贤。概率论及数理统计，第四版，高等教育出版社
